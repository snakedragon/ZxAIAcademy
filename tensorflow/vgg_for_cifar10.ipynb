{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = '/Users/liuyouru/Downloads/cifar-10-batches-py/cifar-10-batches-bin/data_batch_1.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_STEP = 50000\n",
    "BUFFER_SIZE = 256\n",
    "\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = [file]\n",
    "\n",
    "dataset = tf.data.FixedLengthRecordDataset(filename,32*32*3+1)\n",
    "\n",
    "def parse(bin_example):\n",
    "    \n",
    "    decoded = tf.decode_raw(bin_example,out_type=tf.uint8)\n",
    "    x = tf.reshape(decoded[1:],shape=(3,32,32))\n",
    "    x = tf.transpose(x,perm=[1,2,0])\n",
    "    x = tf.image.resize_images(x,[IMAGE_HEIGHT,IMAGE_WIDTH])\n",
    "    x = tf.to_float(x)\n",
    "    \n",
    "    y = decoded[0]\n",
    "    y = tf.to_int32(y)\n",
    "    \n",
    "    return x,y\n",
    "    \n",
    "dataset = dataset.map(parse)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.repeat(-1)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "next_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_op,y_op = next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_16(inputs,num_classes,keep_prob = 0.5):\n",
    "    \n",
    "    end_point = {}\n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d,slim.fully_connected],activation_fn=tf.nn.relu):\n",
    "        \n",
    "        # block1\n",
    "        net = slim.repeat(inputs,2,slim.conv2d,64,[3,3],scope='conv1')\n",
    "        net = slim.max_pool2d(net,[2,2],scope='pool1')\n",
    "        \n",
    "        end_point['block1'] = net\n",
    "        \n",
    "        # block2\n",
    "        net = slim.repeat(net,2,slim.conv2d,128,[3,3],scope='conv2')\n",
    "        net = slim.max_pool2d(net,[2,2],scope='pool2')\n",
    "        \n",
    "        end_point['block2'] = net\n",
    "        \n",
    "        # block3\n",
    "        net = slim.repeat(net,3,slim.conv2d,256,[3,3],scope='conv3')\n",
    "        net = slim.max_pool2d(net,[2,2],scope='pool3')\n",
    "        \n",
    "        end_point['block3'] = net\n",
    "        \n",
    "        # block4\n",
    "        net = slim.repeat(net,3,slim.conv2d,512,[3,3],scope='conv4')\n",
    "        net = slim.max_pool2d(net,[2,2],scope='pool4')\n",
    "        \n",
    "        end_point['block4'] = net\n",
    "        \n",
    "        # block5\n",
    "        net = slim.repeat(net,3,slim.conv2d,512,[3,3],scope='conv5')\n",
    "        net = slim.max_pool2d(net,[2,2],scope='pool5')\n",
    "        \n",
    "        end_point['block5'] = net\n",
    "        \n",
    "        net = slim.flatten(net,scope='flatten')\n",
    "        \n",
    "        net = slim.fully_connected(net,4096,scope='fc6')\n",
    "        end_point['fc6'] = net\n",
    "        net = slim.dropout(net,keep_prob=keep_prob,scope='fc6_drop')\n",
    "        \n",
    "        net = slim.fully_connected(net,4096,scope='fc7')\n",
    "        end_point['fc7'] = net\n",
    "        net = slim.dropout(net,keep_prob=keep_prob,scope='fc7_drop')\n",
    "        \n",
    "        net = slim.fully_connected(net,num_classes,activation_fn=None, scope='fc8')\n",
    "        \n",
    "        end_point['fc8'] = net\n",
    "        \n",
    "        return net,end_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logtis,end_points = vgg_16(x_op,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logtis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.to_int64(y_op),\n",
    "                                                        logits=logtis,\n",
    "                                                        name='cross_entropy')\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "loss_summary = tf.summary.scalar('LOSS',loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logtis,1),tf.to_int64(y_op))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "accuracy_summary = tf.summary.scalar('ACCURACY',accuracy_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all(key=tf.GraphKeys.SUMMARIES)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter('/Users/liuyouru/Downloads/cifar-10-batches-py/logs',\n",
    "                                          tf.get_default_graph())\n",
    "    \n",
    "    init_op.run()\n",
    "    \n",
    "    for step in range(MAX_STEP):\n",
    "        _, cur_loss, cur_accuracy = sess.run([train_op,loss,accuracy_op])\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            summary_str = sess.run(summary_op)\n",
    "            summary_writer.add_summary(summary_str,step)\n",
    "            print('step = ',step,'loss = ',cur_loss,'accuracy',cur_accuracy)\n",
    "            \n",
    "        if step % 1000 == 0:\n",
    "            saver.save(sess,'/Users/liuyouru/Downloads/cifar-10-batches-py/logs/model.ckpt')\n",
    "            \n",
    "    summary_writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
